{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f5fc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorWithPadding, \n",
    "    EarlyStoppingCallback, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import os\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35715e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb8ccbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b35413",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", cache_dir=\"/home/jovyan/work/homeworks/diploma/nllb-train/nllb-200\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\", \n",
    "    src_lang=\"kaz_Cyrl\", \n",
    "    tgt_lang=\"rus_Cyrl\",\n",
    "    cache_dir=\"/home/jovyan/work/homeworks/diploma/nllb-train/nllb-200\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adfd424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"issai/kazparc\", \"kazparc\", cache_dir=\"/home/jovyan/work/homeworks/diploma/nllb-train/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cd41eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda example: example[\"pair\"] == \"kk_ru\")\n",
    "dataset[\"validation\"] = dataset[\"validation\"].filter(lambda example: example[\"pair\"] == \"kk_ru\")\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda example: example[\"pair\"] == \"kk_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c719dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset['train'])\n",
    "valid_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1a0d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_cleaned = train_df.dropna(subset=['source_lang']).drop_duplicates(subset=['source_lang'])\n",
    "valid_df_cleaned = valid_df.dropna(subset=['source_lang']).drop_duplicates(subset=['source_lang'])\n",
    "test_df_cleaned = test_df.dropna(subset=['source_lang']).drop_duplicates(subset=['source_lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b1bc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df_cleaned)\n",
    "valid_dataset = Dataset.from_pandas(valid_df_cleaned)\n",
    "test_dataset = Dataset.from_pandas(test_df_cleaned)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18444d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(\"__index_level_0__\")\n",
    "dataset[\"validation\"] = dataset[\"validation\"].remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b8024d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 286943/286943 [00:50<00:00, 5715.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72413/72413 [00:12<00:00, 5782.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4750/4750 [00:00<00:00, 5785.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 256\n",
    "\n",
    "def tokenize_dataset(example, max_length=MAX_LENGTH):\n",
    "    encodings = tokenizer(\n",
    "        example['source_lang'], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        text_target=example['target_lang'],\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b75ecbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoardCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d75c3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02caf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3065/2101667079.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "model_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./output_dir\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.02,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=model_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    callbacks=[tensorboard_callback],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ea5a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 04:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 12.790704727172852,\n",
       " 'eval_model_preparation_time': 0.0059,\n",
       " 'eval_bleu': 25.311,\n",
       " 'eval_gen_len': 26.8629,\n",
       " 'eval_runtime': 300.6643,\n",
       " 'eval_samples_per_second': 15.798,\n",
       " 'eval_steps_per_second': 0.988}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57153dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/jovyan/work/homeworks/diploma/nllb-train/output_dir/checkpoint-15500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12c3820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3065/2931958843.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 05:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.813709259033203,\n",
       " 'eval_model_preparation_time': 0.0058,\n",
       " 'eval_bleu': 35.727,\n",
       " 'eval_gen_len': 27.9617,\n",
       " 'eval_runtime': 302.7484,\n",
       " 'eval_samples_per_second': 15.69,\n",
       " 'eval_steps_per_second': 0.981}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=model_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    callbacks=[tensorboard_callback],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfdc4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/jovyan/work/homeworks/diploma/nllb-train/output_dir/checkpoint-26901\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e08d986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/venv/low-res/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3065/679023337.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "model_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./output_dir\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.02,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=model_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    callbacks=[tensorboard_callback],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e2b7ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 04:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.492841720581055,\n",
       " 'eval_model_preparation_time': 0.0104,\n",
       " 'eval_bleu': 35.9129,\n",
       " 'eval_gen_len': 27.9634,\n",
       " 'eval_runtime': 300.5336,\n",
       " 'eval_samples_per_second': 15.805,\n",
       " 'eval_steps_per_second': 0.988}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a824cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "pt_13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
